import{_ as a,c as t,o as e,a4 as i}from"./chunks/framework.aImdPBTI.js";const o="/iyanBase/assets/2024-05-12-16-38-38.Rte7UMjn.png",n="/iyanBase/assets/2024-05-12-16-57-27.MRmhH3uO.png",s="/iyanBase/assets/2024-05-12-17-41-43.D4gGckEl.png",r="/iyanBase/assets/2024-05-12-18-00-08.B_Dkt4Ht.png",c="/iyanBase/assets/2024-05-13-00-42-07.DHAKtdYm.png",f=JSON.parse('{"title":"注意力机制","description":"","frontmatter":{},"headers":[],"relativePath":"articles/learning/artificial-intelligence/transformer/QKV.md","filePath":"articles/learning/artificial-intelligence/transformer/QKV.md"}'),p={name:"articles/learning/artificial-intelligence/transformer/QKV.md"},l=i('<h1 id="注意力机制" tabindex="-1">注意力机制 <a class="header-anchor" href="#注意力机制" aria-label="Permalink to &quot;注意力机制&quot;">​</a></h1><p><em>捕捉Q,K之间的关系,来得出结果</em></p><h2 id="_1-对于一维qkv" tabindex="-1">1.对于一维QKV <a class="header-anchor" href="#_1-对于一维qkv" aria-label="Permalink to &quot;1.对于一维QKV&quot;">​</a></h2><p><img src="'+o+'" alt=""></p><h4 id="输入q-与ki的点积得到那些vi需要注意-需要多少的注意-权重-把各个vi-权重a-q-k-得到结果" tabindex="-1"><strong>输入Q,与Ki的点积得到那些Vi需要注意,需要多少的注意(权重),把各个vi*权重a(Q,K)得到结果</strong> <a class="header-anchor" href="#输入q-与ki的点积得到那些vi需要注意-需要多少的注意-权重-把各个vi-权重a-q-k-得到结果" aria-label="Permalink to &quot;**输入Q,与Ki的点积得到那些Vi需要注意,需要多少的注意(权重),把各个vi*权重a(Q,K)得到结果**&quot;">​</a></h4><p><strong>f(q)</strong>,计算<strong>Q</strong>对全局<strong>K</strong>的关联程度,即<code>a(Q,K)</code>,获得<strong>Q</strong>对于全局键值对 **(Q,k)**因为<code>softmax()</code>所以置信度为(0~1)之间.</p><h3 id="_2-高维qkv" tabindex="-1">2. 高维QKV <a class="header-anchor" href="#_2-高维qkv" aria-label="Permalink to &quot;2. 高维QKV&quot;">​</a></h3><p><img src="'+n+'" alt=""></p><h3 id="_3-自注意机制的qkv" tabindex="-1">3.自注意机制的QKV <a class="header-anchor" href="#_3-自注意机制的qkv" aria-label="Permalink to &quot;3.自注意机制的QKV&quot;">​</a></h3><p>普通的点积QKV,其中的KV是给定的,没什么可以训练的地方. 提出自注意力机制</p><p><em>只关心输入X中元素的内在联系;</em></p><ol><li>通过训练(可以是全连接等方法),训练出X对应的WQ,Wk,Wv(注意,这里的三个参数都是根据X自己得出的,如果X为高维(多组输入组成的矩阵),那么每组输入有对应自己的WQ,WK,Wv),来确定自己内部元素之间的关联度,并乘以对应KEy的值,得出答案. <img src="'+s+'" alt=""></li></ol><p>下图为 根据#1得出的Q,计算#1与全局的关联度的方法.即计算根据#1计算出的Q,点积#1,#2,#3经过学习得到的K(即计算与这三者的关联度),再乘对应key的Value,得到output,即#1对于#1,#2,#3的注意力得分. 值得注意的是,对Q1*Ki的点积的softmax(),可以让中注意力权重之和为1.</p><p><u><strong>本质上,是根据#1得出的Q,逐个与序列中的其他项的Ki,vi做QKV方程计算,最终得到#1与整个序列的关联度表output</strong></u><img src="'+r+'" alt=""></p><h3 id="_2-多头注意力机制" tabindex="-1">2.多头注意力机制 <a class="header-anchor" href="#_2-多头注意力机制" aria-label="Permalink to &quot;2.多头注意力机制&quot;">​</a></h3><p>普通自注意力机制的升级版</p><p>在标准的自注意力机制中,对于序列中任意两个单词对,我们计算它们之间的相似性得分,作为注意力权重。<strong>但这种方式对于所有单词对都使用了相同的注意力计算方式,无法针对不同的位置关系(如近邻、远距离、前后关系等)施加不同的注意力模式。</strong></p><p>而在多头注意力中,我们首先将输入通过不同的线性变换映射到不同的子空间表示,相当于为每个&quot;头&quot;生成一个子表示: <code>head_i = W_i * X (i=1...h, h为头数)</code></p><p>其中不同的W_i对应不同的线性变换,会产生不同的子空间表示head_i。 然后在每个head_i上分别计算自注意力,得到不同的注意力分布: <code>Copy codeattention_i = SelfAttention(head_i)</code></p><p>由于不同头对应不同的子空间表示,在计算注意力时就会自动关注不同的位置和语义信息。比如某一头可能更关注局部位置关系,另一头则偏向全局依赖等。</p><p>最后,将所有头的注意力表示合并得到最终输出: <code>Copy codeMultiHead(X) = Concat(attention_1, ..., attention_h) * W_o</code></p><p>其中 W_o 是一个可训练的权重矩阵,用于合并多头信息。</p><p><img src="'+c+'" alt=""> 把原始输入X投影到size(x)/h 的维度上,在低维的X&#39;上列QKV矩阵,得到Zi.最后把Zi用Wo权重的方式合并.</p><p><strong>本质: 给h次机会,让各个头的Wqi,wki,Wvi以不同的随机种子发芽,来希望模型能匹配到不同的模式(类似卷积网络的不同卷积通道)</strong></p><p><strong>ps:把x切h是受到cnn的启发,实际上的实验证明了,减少每个头输入的维度,对于整体的效果也不错.</strong></p>',25),_=[l];function d(h,g,m,u,Q,K){return e(),t("div",null,_)}const k=a(p,[["render",d]]);export{f as __pageData,k as default};
