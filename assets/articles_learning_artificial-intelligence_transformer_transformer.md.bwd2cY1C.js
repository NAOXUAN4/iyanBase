import{_ as t,c as a,a as e,a4 as s,j as r,o as n}from"./chunks/framework.aImdPBTI.js";const o="/iyanBase/assets/2024-05-15-22-50-28.D5Jfbuk0.png",i="/iyanBase/assets/2024-05-15-22-52-58.DkbvkPWQ.png",l="/iyanBase/assets/2024-05-15-23-03-14.BtUYN3xY.png",N=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"articles/learning/artificial-intelligence/transformer/transformer.md","filePath":"articles/learning/artificial-intelligence/transformer/transformer.md"}'),c={name:"articles/learning/artificial-intelligence/transformer/transformer.md"},_=s('<h2 id="transformer" tabindex="-1">transformer <a class="header-anchor" href="#transformer" aria-label="Permalink to &quot;transformer&quot;">​</a></h2><p>概述:使用注意力机制的模型架构<br><br><br></p><ol><li><em>使用encoder_decoder的transformer结构</em><br><img src="'+o+'" alt=""></li></ol><p><strong>适用于语言翻译等要求输入输出严格对应的任务</strong></p><br><br>',6),m=r("p",null,[r("img",{src:i,alt:""}),r("br"),r("strong",null,"无需考虑以前时刻的输出,一次输入输出,即为一次生成的全过程")],-1),d=r("br",null,null,-1),f=r("br",null,null,-1),p=r("p",null,[r("img",{src:l,alt:""}),e(" **只需在以前的输出上不断接龙 **")],-1);function g(u,h,b,B,T,k){return n(),a("div",null,[_,e(" 2. 只是用encoder的结构 (以vit为例) "),m,d,f,e(" 3.只用decoder的结构 (decoder only) "),p])}const P=t(c,[["render",g]]);export{N as __pageData,P as default};
