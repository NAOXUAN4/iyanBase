## transformer  

概述:使用注意力机制的模型架构  
<br />
<br />
1. *使用encoder_decoder的transformer结构*  
![](2024-05-15-22-50-28.png) 

**适用于语言翻译等要求输入输出严格对应的任务** 


<br />
<br />
2. 只是用encoder的结构 (以vit为例)

![](2024-05-15-22-52-58.png)  
**无需考虑以前时刻的输出,一次输入输出,即为一次生成的全过程**  

<br />
<br/>
3.只用decoder的结构 (decoder only) 

![](2024-05-15-23-03-14.png) 
**只需在以前的输出上不断接龙 **
